{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5b2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7b838",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f96e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './aclImdb'\n",
    "glove_path = './glove.6B/'\n",
    "# max number of words in the texts to be vectorized (choose the frequent words)\n",
    "max_nb_words = 20000\n",
    "# max number of words in a review (the review is padded or trucated to the number)\n",
    "num_words_per_review = 1000\n",
    "# glove embedding dimension\n",
    "glove_dim = 100\n",
    "validation_ratio = 0.2\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ab5d59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:08:52\n"
     ]
    }
   ],
   "source": [
    "# load the movie review texts and sentiment labels\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(data_path, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r',encoding=\"utf8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "            \n",
    "df.columns = ['review', 'sentiment']\n",
    "texts = df['review'].values.tolist()\n",
    "labels = df['sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858b6402",
   "metadata": {},
   "source": [
    "#  load the glove vectors\n",
    "# the dictionary for mapping a word to a 100-dim vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa6802fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = {}\n",
    "f = open(os.path.join(glove_path, 'glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    fields = line.split()\n",
    "    word = fields[0] # the first element is the word\n",
    "    word_vector = np.asarray(fields[1:], dtype='float32') \n",
    "    glove_embedding[word] = word_vector\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4becd61",
   "metadata": {},
   "source": [
    "# Tokenization of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e17a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_nb_words) \n",
    "tokenizer.fit_on_texts(texts) \n",
    "# convert each review text into a sequence of word-indices\n",
    "matrix_word_indices = tokenizer.texts_to_sequences(texts)\n",
    "# the dictionary for mapping a word to an index\n",
    "dictionary_word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17ee29",
   "metadata": {},
   "source": [
    "# Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92139f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_word_indices_fixed_length = pad_sequences(matrix_word_indices, maxlen = num_words_per_review) \n",
    "data = np.array(matrix_word_indices_fixed_length)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858c42f",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244aa74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data \n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "# percentage of validation data\n",
    "nb_validation_samples = int(validation_ratio*data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_validation = data[-nb_validation_samples:]\n",
    "y_validation = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8688ea",
   "metadata": {},
   "source": [
    "# Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "442412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_nb_words, len(dictionary_word_index))\n",
    "# embedding_matrix[0] is a all-zero vector representing no word\n",
    "embedding_matrix = np.zeros((num_words+1, glove_dim)) \n",
    "for word, index in dictionary_word_index.items():\n",
    "    if index > max_nb_words:\n",
    "        continue \n",
    "    # get the glove vector for the word\n",
    "    glove_vector = glove_embedding.get(word) \n",
    "    if glove_vector is not None: \n",
    "        embedding_matrix[index] = glove_vector        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2bceac",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1352c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(num_words_per_review,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(num_words+1, glove_dim, weights=[embedding_matrix], input_length=num_words_per_review, trainable=True)\n",
    "embedded_output = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(filters=32, kernel_size=5, activation='relu')(embedded_output)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Conv1D(32, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "final_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "759b3c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "983496c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1000, 100)         2000100   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 996, 32)           16032     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 199, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 199, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 195, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 39, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 39, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1248)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                39968     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,062,341\n",
      "Trainable params: 2,062,341\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cea78d",
   "metadata": {},
   "source": [
    "# fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c8b255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 - 94s - loss: 0.6374 - acc: 0.6091 - mae: 0.4498 - val_loss: 0.4776 - val_acc: 0.7780 - val_mae: 0.3292\n",
      "Epoch 2/5\n",
      "313/313 - 92s - loss: 0.3965 - acc: 0.8304 - mae: 0.2501 - val_loss: 0.3078 - val_acc: 0.8748 - val_mae: 0.2111\n",
      "Epoch 3/5\n",
      "313/313 - 91s - loss: 0.3010 - acc: 0.8783 - mae: 0.1821 - val_loss: 0.2724 - val_acc: 0.8907 - val_mae: 0.1800\n",
      "Epoch 4/5\n",
      "313/313 - 91s - loss: 0.2546 - acc: 0.9000 - mae: 0.1506 - val_loss: 0.2554 - val_acc: 0.9014 - val_mae: 0.1714\n",
      "Epoch 5/5\n",
      "313/313 - 92s - loss: 0.2258 - acc: 0.9132 - mae: 0.1313 - val_loss: 0.2853 - val_acc: 0.8851 - val_mae: 0.1620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x265647bbe20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbar = pyprind.ProgBar(50000)\n",
    "model.fit(x=x_train, y=y_train, validation_data=(x_validation, y_validation), epochs=5, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4a1e9",
   "metadata": {},
   "source": [
    "# Evaluate the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c236f357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2853 - acc: 0.8851 - mae: 0.1620\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = model.evaluate(x_validation, y_validation, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "879edba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c9f9e88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0cf8d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED=list(pred)\n",
    "k=0\n",
    "for i in pred:\n",
    "    if i<0.5:\n",
    "        PRED[k]=0\n",
    "        k+=1\n",
    "    else:\n",
    "        PRED[k]=1\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d76bcd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRED[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "096a0307",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"DLProject.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "78d9252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences([\"ow. A truly exceptional film that somehow lives up to the immense praise. It manages to be so many things at once: an operatic satire, a classist takedown, a cheeky comedy, a twisty thriller, a chilling horror, a somber rumination. Much of the success is owed to the absolutely phenomenal screenplay which is one of the most intricately constructed in recent memory. Every setup is paid off, every tonal shift deftly balanced, every progression carefully timed. The movie feels like a well-oiled machine firing on all cylinders, thrusting you through the densely layered narrative without so much as a hiccup. And perhaps most importantly, despite its highly allegorical nature, the film never compromises on the entertainment value and emotional catharsis of its literal story and characters. It's not often you see a film realize its ambition this resoundingly. Don't miss it.\" ,\n",
    "               \"After all, it's already wasted enough of my time, so I'll be brief. First, this is an R-rated movie...about tag? Maybe if this was naked tag...If that was the case, this would be a very different review. Once again, I should have read the rating before buying it. I didn't do so because it had been marked down in price (for obvious reasons) and bought it impulsively. And at what point did dropping f-bombs into every sentence constitute good writing and movie-making? Assuming that was the objective. There was more swearing in this film than I remember hearing in my last view of 'The Rock' which was a much better film, even if Nic Cage was in it. Decent comedy films apparently can't be made anymore unless there is an inordinate amount of crude language and cuss words. But...this film wasn't even remotely close to decent. I have a massive film collection (everyone needs a hobby) and I've only trashed two films after purchasing and watching them. This one is number three. Don't waste your time on this one; don't rent it, and definitely don't buy it\"])\n",
    "test = pad_sequences(sequence, maxlen=num_words_per_review)\n",
    "x=model.predict(test)\n",
    "for i in x:\n",
    "    if i<0.5:\n",
    "        print('negative')\n",
    "    else:\n",
    "        print(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceab907",
   "metadata": {},
   "source": [
    "# Custom metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "942e237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "def mae(y_true, y_pred):            \n",
    "    eval = K.abs(y_pred - y_true)\n",
    "    eval = K.mean(eval, axis=-1)\n",
    "    return eval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c357764",
   "metadata": {},
   "source": [
    "# building model with custom metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "857f6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(inputs=sequence_input, outputs=final_output)\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer='rmsprop', metrics=['acc',mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3f03dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 - 92s - loss: 0.2028 - acc: 0.9248 - mae: 0.1151 - val_loss: 0.2391 - val_acc: 0.9070 - val_mae: 0.1560\n",
      "Epoch 2/5\n",
      "313/313 - 91s - loss: 0.1754 - acc: 0.9350 - mae: 0.0991 - val_loss: 0.2501 - val_acc: 0.9029 - val_mae: 0.1611\n",
      "Epoch 3/5\n",
      "313/313 - 85s - loss: 0.1558 - acc: 0.9440 - mae: 0.0875 - val_loss: 0.2399 - val_acc: 0.9065 - val_mae: 0.1453\n",
      "Epoch 4/5\n",
      "313/313 - 76s - loss: 0.1409 - acc: 0.9496 - mae: 0.0783 - val_loss: 0.2513 - val_acc: 0.9072 - val_mae: 0.1273\n",
      "Epoch 5/5\n",
      "313/313 - 78s - loss: 0.1238 - acc: 0.9564 - mae: 0.0680 - val_loss: 0.2397 - val_acc: 0.9082 - val_mae: 0.1312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26566204280>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x=x_train, y=y_train, validation_data=(x_validation, y_validation), epochs=5, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bce5568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step - loss: 0.2397 - acc: 0.9082 - mae: 0.1312: 2s - los\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23970139026641846, 0.9082000255584717, 0.13122884929180145]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(x_validation,y_validation,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bcb01174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17112,  2927],\n",
       "       [  468, 19493]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf=confusion_matrix(y_train, PRED)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c853c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
